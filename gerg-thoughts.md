# Project Log: Reinforcement Learning in Trackmania

This document roughly outlines the five major development stages of my reinforcement learning system designed to control a vehicle in Trackmania 2020. Each stages reflects iterative improvements, experiments, and lessons learned across areas including telemetry extraction, model design, reward engineering, and architectural changes.

## Stage 1: Retrieving Game Data and Controlling the Vehicle

The initial goal was to extract real-time game data from Trackmania 2020 and use that data to control the vehicle using Python. This was accomplished through Openplanet, a modding framework that enables custom plugin developement for Trackmania using AngelScript.
To implement the plugin, I wrote an 'info.toml' and a script located in 'plugins/GergBot/'. This plugin adds a menu item in Openplanet that launches a local socket server. The socket transmits the car's current speed, position, velocity and checkpoint count on each update. However, there were two issues identified early:
1. No frame-rate control: Data was sent as fast as possible, leading to inconsistencies in synchronization and strange reward behaviors.
2. TOML structuring confusion: I initially misused the 'meta' and 'script' sections, which are crucial for proper plugin registration.

On the python side, I created a function that runs two threads:
1. One that listens to the socket and updates a global 'latest_state' variable
2. The other makes movement decisions, such as, if speed is below 50 m/s, it presses the gas otherwise, it lets off.

During this phase, I also discovered that 'pynput' holds keys down continuously until explicitly released, which caused unwanted keypress spam when switching windows.

# Stage 2: Initial Machine Learning Model Implementation

https://github.com/user-attachments/assets/5507724a-d6ce-42fa-a510-b82c61f4d2be

During this phase, I restructured my python code. I created a training function, a compute reward function, a policy loss function, and most importantly a genModel function. The genModel function creates a small neural network. it has 8 inputs, based on the values recieved from the angelscript plugin, 2 hidden layers with 512 neurons each, and four outputs, corresponding to the four inputs: forward, backward, left, and right. I also changed how my main calls the functions. I have an inference and a read_game_state function. These two functions are what run in real time to save data on runs for training. The read_game_state listens and parses what comes out of the socket and the inference will save and run that information through the current model. Then after the inference runs and saves enough data, the threads will be closed and training on the data stored during the last epoch will begin. The first successful implementation of the model is in the video shown above. The odd reward calculation was because my python script was not synced with the plugin that I made. This caused some sets of states to not be connected. This led to the vehicle learning that the best way to get rewards was throwing itself over the back of my track so that it would get a large speed.

# Stage 3: Getting the Initial Model to learn

During this phase, the goal was to teach the model how to drive in a straight line. This proved to be a very difficult task, as the model will learn the best way to get the minimal loss even though that loss is meant to help it. For example, I was giving the model a large negative reward if it was slowing down and it learned to just go backwards and got stuck there. Another thing I noticed is that if I do not scale the rewards then my car will just spin in circles as the position is weighted more than the other metrics. One time, before I gave negative rewards for going sideways, the model would just start turning directly to the left or right and get stuck. After all of the issues that I created, I realized that just giving it rewards for going fast and giving it an overall bigger reward for if the whole run does better that it started going further and further. This is shown in the video that will be linked above.

# Stage 4: Adding A Convolutional Neural Network

[![YouTube Video](https://img.youtube.com/vi/-kLVGGpw-KU/0.jpg)](https://youtube.com/watch?v=-kLVGGpw-KU)

During this phase, I did two main things. The first was finding and adding a way to screen capture quickly to get real time game images and the second was switching from Tensorflow to Pytorch. For this I used DXcam, this allowed me to grab a subsection of the screen which I then cut down even more to be a (200, 100) grey scale image. I essentially had two inputs into the model at this point the image and the telemetry data (speed, position, and CP). TensorFlow has a ConvLSTM2D that is not in PyTorch, as such I tried other options like LSTM and 3D convolutions to try to learn spatial awareness from a set of images. I would input a stack of the last 10 frames into the model (later changed to 5 because of space and irrelevance of the images). I realized that the first five were so far behind that keeping it to the last five frames made more sense. During this time, I realized that using tensorflow to run everything on my CPU was too slow and after spending about 6 hours trying to get any form of tensorflow to run on my GPU I decided to switch to Pytorch. When I first made the switch I did not realize that I had to specify for the install to be the cuda one so overall I only had a minor speedup. I rewrote my code to work with pytorch and reinstalled a pytorch version that worked with the cuda version I had. After I got the code to run on the GPU I saw a speedup of 60 going from taking 5 mins for a run to train to about 5 seconds. This allowed for me to focus more on the model and run more runs faster. The only problem was that I was only saving one run at a time and since the movements are so random it could learn those bad runs and get stuck. So, I implemented two important changes. One, was storing more runs per training cycle, the other change was saving the best run. The best run would be the best so far seen by the system that would be stored and added to the other five for the current run. This allowed for the system to actually learn but I realized that the current holdup was not having enough information to train on as the CNN is only seeing so much minimal variations in the track. 

# Stage 5: Lidar and reward struggles

[![YouTube Video](https://img.youtube.com/vi/FMvDgTzFy70/0.jpg)](https://youtube.com/watch?v=FMvDgTzFy70)

During this stage, I removed the CNN because I do not think that I can train the CNN well on just one computer. Instead I delved into a solution that I had seen completed by others working on similar problems. I first tried to do my own math to find how to calculate the values for all of the lines but after spending a few hours and not getting any good progress I scoured the internet to find a paper that had equations for the exact problem that I was working on. The link will be below. In the pictures above you can see a few variations and a video of what I was working on. Once I had implemented a working visualization I plugged the function into a new model file. I changed the reward function a bit and added rewards for staying away from the edges and negative rewards for getting to close to edges. But as with everything, the model learned that the speed reward outweighed hitting walls and so it would directly crash into them. I might go back and add the cnn in to see if having more inputs helps or if it can learn more. I looked into what TMRL did in all of their demonstrations with using the path of a driven run to do well. I did not want to follow exactly in their footsteps with having the reward function be how well it follows the path. However, I did try and implemented the gaussian distribution and tanh that they were using it determine the action value as well as the switching from having two outputs where one was forward, backward and nothing and the other was left, right and nothing. After switching to be more like theirs I am still struggling with getting the reward function to be correct and not leading me to different not ideal ways.
