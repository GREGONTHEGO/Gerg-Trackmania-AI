# Stage 1: Retrieve Data and Move Vehicle

As stated above the goal for the first stage of this problem was to gather information from the game Trackmania 2020 and be able to exercise control on the vehicle such that it moves in game following a python script. The way that I achieved this was through a downloadable modification to the game called Openplanet. This allows the user to code and test plugins for the game. To create a plugin, I needed to code an angelscript file as well as an info.toml that would tell the game to load my script. The script that I used can be found in the 'plugins/GergBot/' folder. The way that the GergBot script works is by creating a menu item on the openplanet modification that will open a local socket. The local socket will then recieve information from the update function. This information includes the speed of the car, the position of the car, the velocity of the car (in x,y, and z directions), and lastly the number of checkpoints that the vehicle has driven through. As of right now, there have been a few errors that I created with this script. The first is that I have not been regulating how often the information is sent to the socket. This led to a funny reward system that I will talk about in a future stage. The second, was that I learned that within a toml file you need to specify what is part of your plugin, 'meta', and what is part of the dependencies, 'script'. On the python side, during this stage I just created some threads. One of the threads was for listening to the socket and editing a global variable latest_state. The other thread was taking the latest_state and if the speed was less than 50 m/s, which is the metric that the games speed is in, it would press the gas and if the speed was greater than 50 m/s then it would step off the gas. Throughout this process I created many memory leaks and realised that the way that the python library that I am using, pynput, is working is that it constantly is holding whatever key I tell it to down until the script stops. This leads to me clicking on another screen and having a ton of ws being left there.

# Stage 2: Initial Machine Learning Model Implementation

https://github.com/user-attachments/assets/5507724a-d6ce-42fa-a510-b82c61f4d2be

During this phase, I restructured my python code. I created a training function, a compute reward function, a policy loss function, and most importantly a genModel function. The genModel function creates a small neural network. it has 8 inputs, based on the values recieved from the angelscript plugin, 2 hidden layers with 512 neurons each, and four outputs, corresponding to the four inputs: forward, backward, left, and right. I also changed how my main calls the functions. I have an inference and a read_game_state function. These two functions are what run in real time to save data on runs for training. The read_game_state listens and parses what comes out of the socket and the inference will save and run that information through the current model. Then after the inference runs and saves enough data, the threads will be closed and training on the data stored during the last epoch will begin. The first successful implementation of the model is in the video shown above. The odd reward calculation was because my python script was not synced with the plugin that I made. This caused some sets of states to not be connected. This led to the vehicle learning that the best way to get rewards was throwing itself over the back of my track so that it would get a large speed.

# Stage 3: Getting the Initial Model to learn

During this phase, the goal was to teach the model how to drive in a straight line. This proved to be a very difficult task, as the model will learn the best way to get the minimal loss even though that loss is meant to help it. For example, I was giving the model a large negative reward if it was slowing down and it learned to just go backwards and got stuck there. Another thing I noticed is that if I do not scale the rewards then my car will just spin in circles as the position is weighted more than the other metrics. One time, before I gave negative rewards for going sideways, the model would just start turning directly to the left or right and get stuck. After all of the issues that I created, I realized that just giving it rewards for going fast and giving it an overall bigger reward for if the whole run does better that it started going further and further. This is shown in the video that will be linked above.

# Stage 4: Adding A Convolutional Neural Network

During this phase, I did two main things. The first was finding and adding a way to screen capture quickly to get real time game images and the second was switching from Tensorflow to Pytorch. For this I used DXcam, this allowed me to grab a subsection of the screen which I then cut down even more to be a (200, 100) grey scale image. I essentially had two inputs into the model at this point the image and the telemetry data (speed, position, and CP). During this time I tried both LSTM and 3D convolutions to try to learn spatial awareness from a set of images. I would input a stack of the last 10 frames into the model. I realized that the first five were so far behind that keeping it to the last five frames made more sense. During this time, I realized that using tensorflow to run everything on my CPU was too slow and after spending about 6 hours trying to get any form of tensorflow to run on my GPU I decided to switch to Pytorch. When I first made the switch I did not realize that I had to specify for the install to be the cuda one so overall I only had a minor speedup. I rewrote my code to work with pytorch and reinstalled a pytorch version that worked with the cuda version I had. After I got the code to run on the GPU I saw a speedup of 60 going from taking 5 mins for a run to train to about 5 seconds. This allowed for me to focus more on the model and run more runs faster. The only problem was that I was only saving one run at a time and since the movements are so random it could learn those bad runs and get stuck. So, I implemented two important changes. One, was storing more runs per training cycle, the other change was saving the best run. The best run would be the best so far seen by the system that would be stored and added to the other five for the current run. This allowed for the system to actually learn but I realized that the current holdup was not having enough information to train on as the CNN is only seeing so much minimal variations in the track. 

# Stage 5: Lidar and reward struggles

During this stage, I removed the CNN because I do not think that I can train the CNN well on just one computer. Instead I delved into a solution that I had seen completed by others working on similar problems. I first tried to do my own math to find how to calculate the values for all of the lines but after spending a few hours and not getting any good progress I scoured the internet to find a paper that had equations for the exact problem that I was working on. The link will be below. In the pictures above you can see a few variations and a video of what I was working on. Once I had implemented a working visualization I plugged the function into a new model file. I changed the reward function a bit and added rewards for staying away from the edges and negative rewards for getting to close to edges. But as with everything, the model learned that the speed reward outweighed hitting walls and so it would directly crash into them. I might go back and add the cnn in to see if having more inputs helps or if it can learn more.
